/home/oza/.pyenv/versions/oza2/lib/python3.7/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/home/oza/.pyenv/versions/oza2/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
56000
14000
Epoch: 0 train_loss: 175.4357147216797
Epoch: 0 test_loss: 139.98501586914062
Epoch: 1 train_loss: 130.74473571777344
Epoch: 1 test_loss: 124.56282043457031
Epoch: 2 train_loss: 121.25621795654297
Epoch: 2 test_loss: 119.13140106201172
Epoch: 3 train_loss: 117.1751937866211
Epoch: 3 test_loss: 115.98776245117188
Epoch: 4 train_loss: 114.77631378173828
Epoch: 4 test_loss: 113.92838287353516
Epoch: 5 train_loss: 113.0941390991211
Epoch: 5 test_loss: 112.85182189941406
Epoch: 6 train_loss: 111.78838348388672
Epoch: 6 test_loss: 111.68526458740234
Epoch: 7 train_loss: 110.49742126464844
Epoch: 7 test_loss: 110.27070617675781
Epoch: 8 train_loss: 109.51950073242188
Epoch: 8 test_loss: 109.56797790527344
Epoch: 9 train_loss: 108.759033203125
Epoch: 9 test_loss: 109.06983184814453
Epoch: 10 train_loss: 108.10089111328125
Epoch: 10 test_loss: 108.57067108154297
Epoch: 11 train_loss: 107.60874938964844
Epoch: 11 test_loss: 107.80960845947266
Epoch: 12 train_loss: 107.11778259277344
Epoch: 12 test_loss: 107.7465591430664
Epoch: 13 train_loss: 106.73960876464844
Epoch: 13 test_loss: 107.4370346069336
Epoch: 14 train_loss: 106.36918640136719
Epoch: 14 test_loss: 107.10741424560547
Epoch: 15 train_loss: 106.06574249267578
Epoch: 15 test_loss: 106.8650894165039
Epoch: 16 train_loss: 105.88087463378906
Epoch: 16 test_loss: 106.41495513916016
Epoch: 17 train_loss: 105.56549835205078
Epoch: 17 test_loss: 106.18782043457031
Epoch: 18 train_loss: 105.41455078125
Epoch: 18 test_loss: 106.0758056640625
Epoch: 19 train_loss: 105.14071655273438
Epoch: 19 test_loss: 105.97440338134766
Epoch: 20 train_loss: 105.01789093017578
Epoch: 20 test_loss: 105.947509765625
Epoch: 21 train_loss: 104.80673217773438
Epoch: 21 test_loss: 105.55912017822266
Epoch: 22 train_loss: 104.64356994628906
Epoch: 22 test_loss: 105.34318542480469
Epoch: 23 train_loss: 104.50725555419922
Epoch: 23 test_loss: 105.29908752441406
Epoch: 24 train_loss: 104.36056518554688
Epoch: 24 test_loss: 105.14464569091797
Epoch: 25 train_loss: 104.22248840332031
Epoch: 25 test_loss: 105.01398468017578
Epoch: 26 train_loss: 104.09966278076172
Epoch: 26 test_loss: 105.30440521240234
Epoch: 27 train_loss: 103.95667266845703
Epoch: 27 test_loss: 105.11891174316406
Epoch: 28 train_loss: 103.91020202636719
Epoch: 28 test_loss: 105.06878662109375
Epoch: 29 train_loss: 103.74954986572266
Epoch: 29 test_loss: 104.87109375
Epoch: 30 train_loss: 103.65567016601562
Epoch: 30 test_loss: 104.65719604492188
Epoch: 31 train_loss: 103.54721069335938
Epoch: 31 test_loss: 104.96892547607422
Epoch: 32 train_loss: 103.44645690917969
Epoch: 32 test_loss: 104.45763397216797
Epoch: 33 train_loss: 103.35273742675781
Epoch: 33 test_loss: 104.46600341796875
Epoch: 34 train_loss: 103.29545593261719
Epoch: 34 test_loss: 104.24821472167969
Epoch: 35 train_loss: 103.17792510986328
Epoch: 35 test_loss: 104.64273071289062
Epoch: 36 train_loss: 103.06196594238281
Epoch: 36 test_loss: 104.38406372070312
Epoch: 37 train_loss: 102.99644470214844
Epoch: 37 test_loss: 104.01651763916016
Epoch: 38 train_loss: 102.93599700927734
Epoch: 38 test_loss: 103.93551635742188
Epoch: 39 train_loss: 102.8641128540039
Epoch: 39 test_loss: 104.05233001708984
Epoch: 40 train_loss: 102.84119415283203
Epoch: 40 test_loss: 104.07047271728516
Epoch: 41 train_loss: 102.7092514038086
Epoch: 41 test_loss: 104.09284973144531
Epoch: 42 train_loss: 102.63825225830078
Epoch: 42 test_loss: 103.97167205810547
Epoch: 43 train_loss: 102.64330291748047
Epoch: 43 test_loss: 103.96627807617188
Epoch: 44 train_loss: 102.56756591796875
Epoch: 44 test_loss: 103.96759796142578
Epoch: 45 train_loss: 102.50840759277344
Epoch: 45 test_loss: 103.69535064697266
Epoch: 46 train_loss: 102.4432144165039
Epoch: 46 test_loss: 103.61992645263672
Epoch: 47 train_loss: 102.39749908447266
Epoch: 47 test_loss: 103.65471649169922
Epoch: 48 train_loss: 102.34585571289062
Epoch: 48 test_loss: 103.72038269042969
Epoch: 49 train_loss: 102.28813934326172
Epoch: 49 test_loss: 103.95437622070312
