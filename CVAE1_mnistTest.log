/home/oza/.pyenv/versions/oza2/lib/python3.7/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/home/oza/.pyenv/versions/oza2/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
56000
14000
Epoch: 0 train_loss: 184.03103637695312
Epoch: 0 test_loss: 166.6276092529297
Epoch: 1 train_loss: 162.44923400878906
Epoch: 1 test_loss: 158.63059997558594
Epoch: 2 train_loss: 156.26942443847656
Epoch: 2 test_loss: 153.8776397705078
Epoch: 3 train_loss: 152.49395751953125
Epoch: 3 test_loss: 150.71861267089844
Epoch: 4 train_loss: 149.80613708496094
Epoch: 4 test_loss: 148.5652618408203
Epoch: 5 train_loss: 147.88597106933594
Epoch: 5 test_loss: 147.21902465820312
Epoch: 6 train_loss: 146.40968322753906
Epoch: 6 test_loss: 146.23403930664062
Epoch: 7 train_loss: 145.39080810546875
Epoch: 7 test_loss: 145.0762176513672
Epoch: 8 train_loss: 144.53797912597656
Epoch: 8 test_loss: 144.59580993652344
Epoch: 9 train_loss: 143.83885192871094
Epoch: 9 test_loss: 143.9019317626953
Epoch: 10 train_loss: 143.18130493164062
Epoch: 10 test_loss: 143.27874755859375
Epoch: 11 train_loss: 142.73524475097656
Epoch: 11 test_loss: 143.5499725341797
Epoch: 12 train_loss: 142.263671875
Epoch: 12 test_loss: 142.86679077148438
Epoch: 13 train_loss: 141.7481231689453
Epoch: 13 test_loss: 142.3982696533203
Epoch: 14 train_loss: 141.5675811767578
Epoch: 14 test_loss: 142.1655731201172
Epoch: 15 train_loss: 141.03231811523438
Epoch: 15 test_loss: 141.67440795898438
Epoch: 16 train_loss: 140.72836303710938
Epoch: 16 test_loss: 141.78793334960938
Epoch: 17 train_loss: 140.58790588378906
Epoch: 17 test_loss: 141.1544952392578
Epoch: 18 train_loss: 140.34588623046875
Epoch: 18 test_loss: 140.9781951904297
Epoch: 19 train_loss: 140.03231811523438
Epoch: 19 test_loss: 141.11868286132812
Epoch: 20 train_loss: 139.77035522460938
Epoch: 20 test_loss: 141.0647430419922
Epoch: 21 train_loss: 139.57591247558594
Epoch: 21 test_loss: 140.64085388183594
Epoch: 22 train_loss: 139.63455200195312
Epoch: 22 test_loss: 140.72543334960938
Epoch: 23 train_loss: 139.2435760498047
Epoch: 23 test_loss: 141.13722229003906
Epoch: 24 train_loss: 139.3802032470703
Epoch: 24 test_loss: 140.2311248779297
Epoch: 25 train_loss: 138.86744689941406
Epoch: 25 test_loss: 139.91683959960938
Epoch: 26 train_loss: 138.84640502929688
Epoch: 26 test_loss: 140.24534606933594
Epoch: 27 train_loss: 138.69549560546875
Epoch: 27 test_loss: 139.88677978515625
Epoch: 28 train_loss: 138.51902770996094
Epoch: 28 test_loss: 139.95994567871094
Epoch: 29 train_loss: 138.4474334716797
Epoch: 29 test_loss: 139.72166442871094
Epoch: 30 train_loss: 138.3193817138672
Epoch: 30 test_loss: 139.5294189453125
Epoch: 31 train_loss: 138.07383728027344
Epoch: 31 test_loss: 139.74839782714844
Epoch: 32 train_loss: 138.1376190185547
Epoch: 32 test_loss: 139.4103546142578
Epoch: 33 train_loss: 138.0308074951172
Epoch: 33 test_loss: 139.37246704101562
Epoch: 34 train_loss: 137.7450714111328
Epoch: 34 test_loss: 139.09060668945312
Epoch: 35 train_loss: 137.8240966796875
Epoch: 35 test_loss: 139.0152130126953
Epoch: 36 train_loss: 137.73760986328125
Epoch: 36 test_loss: 139.3318328857422
Epoch: 37 train_loss: 137.65267944335938
Epoch: 37 test_loss: 139.10743713378906
Epoch: 38 train_loss: 137.43600463867188
Epoch: 38 test_loss: 138.92295837402344
Epoch: 39 train_loss: 137.37025451660156
Epoch: 39 test_loss: 138.75558471679688
Epoch: 40 train_loss: 137.49717712402344
Epoch: 40 test_loss: 138.92440795898438
Epoch: 41 train_loss: 137.44781494140625
Epoch: 41 test_loss: 138.4926300048828
Epoch: 42 train_loss: 137.38912963867188
Epoch: 42 test_loss: 138.82154846191406
Epoch: 43 train_loss: 137.13755798339844
Epoch: 43 test_loss: 138.52984619140625
Epoch: 44 train_loss: 136.92396545410156
Epoch: 44 test_loss: 139.2000274658203
Epoch: 45 train_loss: 136.9729461669922
Epoch: 45 test_loss: 138.43862915039062
Epoch: 46 train_loss: 136.9226531982422
Epoch: 46 test_loss: 138.87550354003906
Epoch: 47 train_loss: 136.8418426513672
Epoch: 47 test_loss: 138.4261016845703
Epoch: 48 train_loss: 136.66212463378906
Epoch: 48 test_loss: 138.4407958984375
Epoch: 49 train_loss: 136.69667053222656
Epoch: 49 test_loss: 138.36276245117188
